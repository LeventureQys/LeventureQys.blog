# 机器学习，从零开始

本博客页面将记录我从零开始机器学习的一些笔记和启发

## part1 绪论

1.1 浅谈一些基础概念

首先我们学习的时候要知道一些基础概念：

  1.数据集：记录的集合
  
  2.实例、样本：就是关于一个时间或对象的描述
  
  3.属性、特征：反应时间或对象在某方面的表现或者性质的事项，例如：“色泽”
  
  4.属性值： 某个事物或对象在属性上的取值，比如“乌黑”
  
  5.样本空间、属性空间、输入控件：属性张成的空间。
  
以上几个是基本概念，后续应该就不会再介绍了。

![image](https://user-images.githubusercontent.com/102945300/173177667-6975382a-6e5c-46d6-8557-f8092dc51aaf.png)

我们让机器从数据中学得模型的过程成为 学习 或者 训练 ，这个过程通过执行某个学习算法来完成。

训练用的数据成为训练数据，其中的每个样本成为训练样本。

由训练样本组成的集合成为 训练集

学的模型对应了关于数据的魔偶中潜在规律 称为  假设

而这种规律本身我们称之为 真相 或者 真实，其中整个学习过程就是为了找到或者说逼近真相。 

模型又称作学习器 learner，可看作学习算法在给定数据和参数空间上的实例化。

我们一般用![image](https://user-images.githubusercontent.com/102945300/173177808-9e86a53d-1c1c-4230-b4d4-151a95458c3e.png)表示第i个样例![image](https://user-images.githubusercontent.com/102945300/173177816-a81404c7-126f-41e7-9ea7-823322a2359a.png)

这个标记其实就是我们给定的 结果 ，比如好瓜 坏瓜，或者成熟度0.9，成熟度0.8这类的结果

如果预测离散值，我们称这种任务叫分类，如果是连续的，则称其为回归

书中提到结论：总误差和算法无关，但是不是绝对的，就比如我们可以从A地到B地，我们可以骑单车 或者打的去，但是如果从A省到B省......我们当然还是可以打的去，或者骑单车，但是这都不可能是我们想要的答案。所以要根据不同的情境来决定不同的学习偏好，这样才能让我们得到一个更好的结果，这也是机器学习的研究方向之一。

这里是第一张绪论的习题与答案：[](https://zhuanlan.zhihu.com/p/355235881?ivk_sa=1024320u)


## part2 模型评估与选择

### 1.精度

首先书上给出精度的概念

![image](https://user-images.githubusercontent.com/102945300/173178860-bf0d5061-cb66-47ec-b8e5-59343546ca8e.png)

我们说训练误差有两种可能，一种是学习器能力低下，丢失了很多实例的细节导致的 欠拟合 ，但是欠拟合很好解决，真正的关键障碍是学习机的学习能力过于强大导致的欠拟合。以至于把训练样本所包含的不太一般的特性都学到了，也可能导致一些舍本逐末的误差。

![image](https://user-images.githubusercontent.com/102945300/173178935-836bc42b-f8ec-4064-9139-253b3c4521d4.png)

过拟合是不可避免的，其实机器学习很多问题都是NP-Hard问题，关于什么是NP问题请参考[NP问题总结（概念+例子+证明）](https://blog.csdn.net/a12638915/article/details/105180347)

也就是说机器学习中关于模型的选择和筛选其实本质上是NP问题复杂度更高的问题，这也是不可避免的。

###　2.评估方法

我们评估的时候，会有两个已知结果的集，一个是训练集，用来训练我们的学习器，另一个是测试集，用来测试我们学习器的学习成果的，二者最好能做到尽可能互斥。我们当然是希望得到一个泛化性更强的模型，这样可以更加的反应我们学习器的学习成果，避免一些重复的经验影响到判断的结果。

当然，我们一般情况下都只会拿到一个数据集来进行训练，划分测试集和训练集一般有几种常见的做法，这里截图一下书上怎么说的

2.1留出法 

![image](https://user-images.githubusercontent.com/102945300/173179289-c7de8f87-1c8c-443d-a32b-790301eb19e3.png)

当然这样有可能导致一些问题：两个集数量严重不一致，可能S占了绝大多数，T占了绝小部分，这样就会对我们的实际测试造成很大的影响。

![image](https://user-images.githubusercontent.com/102945300/173179366-5013eada-e959-43c2-afd9-b9400a1b4623.png)

2.2交叉验证法：

就是把数据集D划分为k个大小差不多的互斥子集，然后选则其中k-1个为训练集，剩下的哪个做测试机，从而可以进行k次训练和测试

![image](https://user-images.githubusercontent.com/102945300/173179765-ba428eb3-afb3-4d50-8ced-b4ac4d49e1e6.png)

![image](https://user-images.githubusercontent.com/102945300/173179958-53ccd568-4ad5-4d3f-a4bb-4fc98b95effc.png)

![image](https://user-images.githubusercontent.com/102945300/173179963-3bffac2a-811b-48e4-8436-f6c555ac0a44.png)

2.3自助法：

我感觉它书上讲的好复杂，就是我们现在视一个总的数据集D，一个训练集集为D‘,测试集为D/D’。

我们将总的数据集D划分为m的个小的集合，然后每次我们从中取一个集合拷贝进入D'，然后我们将这个集合放回，这样我们下次采样的时候还是有可能采样到m对吧。执行m次后，我们就得到了包含m个样本的数据集D'。显然D中的样本只会有一部分出现在D'中，而另一部分不出现，计算公式如下：

![image](https://user-images.githubusercontent.com/102945300/173180387-1db32385-206d-45f8-8426-158336121c44.png)

也就是说D中有大约0.368的样本没有出现在D'中，而这部分也就是D/D'，所以我们将大的部分也就是D'占D的1-0.368作为训练集，小的部分也就是D/D'占0.368做测试集

这个方法其实有点傻逼，而且能效比说实在的比较低。用在数据集较小，难以有效划分训练集测试集的时候有用，但是量大了就是真的抽象。除此之外，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差。

2.4宇宙的终极尽头：调参

![image](https://user-images.githubusercontent.com/102945300/173180657-9aff43af-bf8b-4b24-b231-d3a5754545e1.png)


### 3.性能度量：

最常用的性能度量就是均方误差，当然不是把每个实例的误差相加求平均就行了，要求各均方误差

![image](https://user-images.githubusercontent.com/102945300/173180755-51aa70d9-3a9a-4783-ae9a-2ae1338977bf.png)

就是学习出来的结果和实际结果之间的差求平方，再求个平均数，这个就是我们学习效果的均方误差

当然了如果是数据分布和概率密度函数的，那么我们的均方误差可以描述为上述

下面介绍一些常用的性能度量

3.1错误率和精度

以下定义一个错误率和精度

![image](https://user-images.githubusercontent.com/102945300/173180820-08f73c65-ef79-46fc-a7ed-42ea377ff202.png)

一个更一般的数据分布和概率精度函数：

![image](https://user-images.githubusercontent.com/102945300/173180963-12b7442d-586b-4bc3-b25c-c36f83fe3657.png)

3.2查准率、查全率和F1

![image](https://user-images.githubusercontent.com/102945300/173181217-2bcf1c3d-1470-40fc-92d9-8bce474b0e65.png)

我们在实际的学习中，预测的结果应该是有四类

![image](https://user-images.githubusercontent.com/102945300/173181231-acc6495c-4732-47ef-90a8-873fbd0c7a81.png)

夏然TP+FP+TN+FN=样例总数

![image](https://user-images.githubusercontent.com/102945300/173181248-e2c158c5-9c9b-4eab-a523-333388c70530.png)

二者都是只需要看真正例做分母，区别在于查准是以真正例和假正例做分母，而查全则是真正例和假反例做分母。一般来说查准率高那么查全率往往就偏低，而查全率高的时候，查准率往往就低。

也就是说，我们希望查询的对象尽可能多，那么查到我们需要的对象的几率自然就低了；如果我们希望查询的对象尽可能是我们需要的对象，那么南美就会漏掉不少的查询对象，这两种度量指标都要依靠我们的需要来选取，并不是说高了就一定好。

比如我们在瓜堆里找西瓜，有两种理想状态：1.希望尽可能地把好西瓜尽可能多地选出来，我们就会增加选瓜的数量2.如果我们希望尽可能地在有限的次数中选中好瓜的几率尽可能高，那么我们就会偏向挑选更有把我的瓜，但这样难免会漏掉不少的好瓜，使得查全率较低。通常只有在一些简单的任务，或者好瓜数量较多的任务里，查全率和查准率才有可能都会高。

注：正 反 是一个相对的概念，而真 假是一个绝对的概念，这里描述一下什么是真正例，什么是假反例

正、反指的是与我们预测的方向是否相同，而真假指的是预测结果是否与真是情况相同。

![image](https://user-images.githubusercontent.com/102945300/173181574-095968a3-1e24-40fc-9106-812be89f4bc2.png)

查准率P，查全率R

![image](https://user-images.githubusercontent.com/102945300/173181734-e7162cff-6ebb-4373-94d6-856abaa13fc8.png)

这个曲线一般就是谁更朝外谁就更优，当然只有完全包住的情况，如果二者有交叉，那就要视情况而定了，不同的需求会决定不同的评判标准。

当然我们也可以去比较面积，但是这个值还是不太好估算，所以我们设计了一些指标。

1.平衡点BEP：也就是查准率=查全率时的取值，比如上图中C的BEP时0.64，基于BEP比较，就可以说A比B好，但是BEP太简单了，所以有时候我们常用的是F1度量：

![image](https://user-images.githubusercontent.com/102945300/173181884-e535899f-2ec1-49b6-bb95-2f29ecf3342e.png)

当然了，我们在不同的情境下对查准率和查全率的重视程度又不尽相同，所以我们给出了一个F1的一般形式Fβ

![image](https://user-images.githubusercontent.com/102945300/173181912-39075334-4776-4e01-86ed-30fb0c223e99.png)

2.3.3 ROC和AUC

1.ROC:受试者工作特征曲线

ROC曲线则是以真正例率TPR为纵轴，假正例率FPR为横轴

其中，真假正例率计算法则如下：

![image](https://user-images.githubusercontent.com/102945300/173182097-f56d1b48-1384-489f-9b19-d21d254210ee.png)

这里其实这么理解：真正例率就是真正例/所有正例（包括真正例+假反例），假正例率就是 假反例/所有反例(真反例+假正例)

由上我们可以画出曲线如图：

![image](https://user-images.githubusercontent.com/102945300/173182303-ed2e0a8f-7025-4b62-b56a-32ee54a0cb47.png)

如何绘制ROC曲线？

![image](https://user-images.githubusercontent.com/102945300/173182449-1d328637-92c3-4b5d-b756-4bd8376c24c6.png)

同样的，如果其中一个ROC曲线可以包住另一个曲线，那么我们称前面的这个学习器的性能优于后者，如果发生交叉了也很难断言，但如果一定要比较，我们比较ROC曲线下的面积，即AUC即可

![image](https://user-images.githubusercontent.com/102945300/173182490-d73a19d7-e153-4383-b0fd-a3552fc4931b.png)

其实这个公式就是我们很熟悉的积分公式的变体，不过是离散状态下的罢了。

2.3.4 代价敏感错误率和代价曲线

就我们实际情况下有时候对判对判错的后果不一样，可能判真正例对了给五毛，但是判出真反例了就要罚一百万，就比如我们如果是一个医生，宁可多查，不能漏查一个道理：多查最多就是花点钱，浪费点时间，但是如果漏查那可能就会导致病人错过最佳救治时间，那就死啦死啦滴了。这就要求我们对二者赋予不同的代价敏感错误率。

由此我们就可以根据上述情况给出一个代价矩阵，以一个比较经典的二分类代价矩阵为例。注：我们一般情况下看重的是比值，绝对值的大小含义并不大。

![image](https://user-images.githubusercontent.com/102945300/173182792-c8eecf96-0627-4825-a95b-f3a5c0bf9b45.png)

比如我们令cost01 = 1, cost10 = 5，也就是说我们把0判成了1就会有1的代价，但是如果我们把1看成了0就会有5的代价。我们在实际的运行和设计算法的时候当然都希望总体代价最小，并以此作为设计偏好，影响计算机在实际情况下的判别。当然了，我们在前面的性能度量中都默认了代价是均等的，并没有考虑过不同的错误带来的结果不同，在此我们需要声明的是，我们所希望的并不是简单地最小化错误次数，而是希望最小化总体代价。

如果我们对上图，以0为正，1为反，则我们可以计算代价敏感 下的错误率为：

其中D+与D-分别代表样例集D的正例子集和反例子集

![image](https://user-images.githubusercontent.com/102945300/173182961-df940e11-3304-4bca-8cc4-97c98d3af5ed.png)

也就是说我们在计算错误率的时候多了一个权值cost

当然了，如果代价非均等了，那么显然ROC曲线也不能直接反映出学习器的期望总体代价了，那么我们还是需要给出新的公式：

![image](https://user-images.githubusercontent.com/102945300/173185853-7fcb0602-ae18-41bc-8c6c-aba51d3e5f4a.png)

![image](https://user-images.githubusercontent.com/102945300/173185858-a9e60307-c373-4fb4-911d-be04aa0ef18b.png)

### 4 比较检验

ok，我们上面就给出了评估方法和性能度量，那么我们现在就可以对学习器的性能进行评估比较了：先使用某种实验评估方法（指我们划分测试集和训练集的划分方法）测得学习器的某个性能度量结果，然后对这些结果进行比较（测试性能度量）。

但我们怎么比较呢？直接对某个值比大小吗？当然实际情况肯定比这样复杂的多，有几个重要因素：

1.我们希望比较的是泛化性能，然而通过实验评估方法我们获得的仅仅是学习成果在测试集上的性能，测试环境和实际环境大概率不是一致的

2.测试集的性能和测试集本身的选择有很大的关系，且不论使用不同的测试集会得到不同的结果，即使使用相同大小的测试集，样例不同，结果也不同，这也是显而易见的。

3.很多机器学习算法本身就有一定的随机性，即使用相同的参数设置在同一个测试集上多次运行，其结果也会有所不同。

综上，单独的数据比较是没有实际含义的，所以我们要用 统计假设检验 为我们进行学习器性能比较托底：

![image](https://user-images.githubusercontent.com/102945300/173186115-9c1ceff9-02be-4fcd-a89f-90ac16e131a2.png)

4.1 假设检验：
